#  HitAnomaly
I OBJECTIVE 

Logs are frequently used in operating systems and other software systems to document important events and system status. Finding irregularities in these logs is essential for efficient system maintenance and service administration. Log event indexes that are generated from the log data are a key component of several existing anomaly detection techniques in system logs. These methods, however, perform poorly with previously encountered log templates, which results in less precise anomaly detection. While delving into the meaning of log templates, some recent research have failed to recognise the importance of parameter values. As a result, we are implementing the same log-based anomaly detection model called "HitAnomaly" which overcomes the drawbacks of current approaches [1]. Research Analysis: The paper [1] investigates the effectiveness of a transformer-based model called "HitAnomaly" for log-based anomaly detection, comparing it to traditional methods like LSTM. This comparison is important because anomalies in logs may have subtle patterns that traditional methods struggle to detect. The research focuses on integrating log templates and parameter values to capture the full context of log sequences. The study aims to determine if HitAnomaly outperforms traditional methods and deep learning models [3] (LSTM) in log-based anomaly detection to assess its potential impact. Additionally, the research evaluates how well HitAnomaly performs on log data with previously unseen events or sequences, which is crucial for practical applications where log data can evolve over time.


II. DATASET

A. Source and type of data
The dataset used here is Hadoop Distributed File System (HDFS), which is commonly used in big data processing and storage. The dataset is related to system log data, which typically contain information about events, components, and parameter values. Each row in the dataset represents a log entry generated by a computer system or application. These log entries are in free-text format, which are preprocessed and encoded.

B. Number of Data Points: There are 5 Data points,These data points typically include various fields or attributes, such as:
• Component: The system component generating the log entry.
• Event Template: A template representing the log's content, excluding specific parameters or variable data.
• Content: Additional contextual information related to the log entry.
• Timestamp: The time when the log entry was recorded.
• Level: The severity or importance level of the log entry.


C. Why this dataset is representative? The HDFS 2k log dataset provides an accurate portrayal of the operations and system events in the Hadoop Distributed File System (HDFS). It is packed with a multitude of log entries, each serving as a data point, that span a wide array of system actions, failures, and warnings that are typically associated with HDFS operations.

III. METHODOLOGY
A. Overall Implementation plan:
We will be following methodology outlined in the reference document, which can be describes as follows:
• Data Preprocessing: First, structured log data is processed to extract log templates and parameter values, discarding timestamps. This stage cleans and organises data for processing.
• Log Sequence and Parameter Value Encoding: The log sequences [2] are encoded using a hierarchical transformer structure that captures semantic information. Parameter values are also encoded, considering their correlation with log templates to capture the context of parameter values within log sequences.
• Anomaly Detection Model: The encoded log sequences and parameter values are input into the HitAnomaly model, which employs a transformer network. This
network is designed to detect anomalies by understanding the semantic information and the hierarchical nature of log data.
• Model Training: The training process involves adjusting the model to minimize the loss and improve its ability to detect anomalies accurately - Instead of this say, once the initial model training is done, we will continue to fine-tune the model parameters through multiple iterations to find the optimal hyper parameters (also known as HyperParameter Fine-Tuning).
• Evaluation: The model's performance is evaluated on both stable and unstable log data. Metrics such as precision, recall, and F1-score are used to assess the model's accuracy in detecting anomalies.
• Fig 1: Implementation Plan The upcoming section will explain in detail implementation of each of the above modules.

B. Implementation plan for Log Sequence Encoding & Methodology followed.
Fig 2: Implementation plan for log sequence encoding The "Log Sequence Encoder" above handles system logs for anomaly detection using a hierarchical transformer. Each log word is turned into a vector using one-hot encoding, then transformed into a continuous vector representation using an embedding matrix initialised by a pre-trained Bert model. Each word's importance in the log sequence is determined by self-attention scores on the vector sequence. A scaled dot-product of Query and Key matrices normalised by SoftMax yields these scores. To add non-linearity, the vectors are transmitted via a Feed-Forward Network with ReLU activation. A final average pooling layer aggregates the sequence's contextual information into a fixed-dimension vector for downstream anomaly identification.

C. Implementation Plan for Parameter Value Encoding
Fig3 : Implementation Plan for Parameter Value Encoding
Step1 involves collecting log template input parameters. In Step 2, use Para_Encoder to vectorize each parameter. Average pooling is done on encoded parameter vectors in Step 3. Step 4 uses the 'Log_Encoder' to encode log templates while arguments are encoded. Log template representation RL1 is coupled with parameter representation Rv1 in Step 5. A linear layer, trainable parameters, and bias compose each parameter group's representation. Finally, in Step 6, average pooling is applied to all representations to create Rp.

D. Implementation plan for Attention Based Classification
Fig 4: Implementation plan for Attention based classification.
Step three of the attention mechanism weights the two representations by importance.Calculate each representation's weight with these equations. Alpha_s and alpha_p indicate how much to focus on each representation.The weighted combination of the two representations is run through a SoftMax function to produce a probability distribution over the potential classes or outcomes.

E. Classifier and Training
Finally, we feed our classifier the output from the previous phase. The attention-weighted combination of the two starting representations will determine the classification outcome for
this classifier. SGD with cross-entropy as the loss function trains the model.
Finally,calculates accuracy of the Hitanamoly model, precision, recall, and F1-score for anomaly detection.

EVALUATION METRICS
Metrics Used
Rationale
Accuracy
an overall measurement of the model, which is necessary, sometimes it is also the efficiency of the model
Precision
Precision measures the proportion of correctly identified anomalies among all predicted anomalies
Recall
recall measures the model's ability to identify all true anomalies, which is essential in anomaly detection scenarios
F1-Score
F1-Score combines precision and recall, providing a balanced assessment of the model's performance
